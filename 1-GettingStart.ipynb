{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Getting Start\r\n",
    "tensorflow2基础"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 tensor张量\r\n",
    "Tensorflow中，使用张量作为数据的基本单位。Tensorflow的张量在概念上等同于多维数组，我们可以使用它来描述数学中的标量（零维数组），向量vector（一维数组），矩阵matrix（二维数组）等。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 定义一个标量（随机数）\r\n",
    "random_float = tf.random.uniform(shape=())\r\n",
    "#定义一个有两个元素的零向量\r\n",
    "zero_vector = tf.zeros(shape=(2))\r\n",
    "#定义2个2*2的常量矩阵\r\n",
    "A = tf.constant([[1,2],[3,4]])\r\n",
    "B = tf.constant([[5,6],[8,9]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取tensor相关属性"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#查看形状\r\n",
    "print(A.shape)\r\n",
    "#查看数据类型\r\n",
    "print(A.dtype)\r\n",
    "#查看值\r\n",
    "print(A.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2)\n",
      "<dtype: 'int32'>\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensor运算"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#和\r\n",
    "C = tf.add(A, B)\r\n",
    "print(C.shape)\r\n",
    "print(C.dtype)\r\n",
    "print(C.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2)\n",
      "<dtype: 'int32'>\n",
      "[[ 6  8]\n",
      " [11 13]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#乘积（矩阵相乘）\r\n",
    "print(A)\r\n",
    "print(B)\r\n",
    "D = tf.matmul(A, B)\r\n",
    "print(D.shape)\r\n",
    "print(D.dtype)\r\n",
    "print(D.numpy()) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[5 6]\n",
      " [8 9]], shape=(2, 2), dtype=int32)\n",
      "(2, 2)\n",
      "<dtype: 'int32'>\n",
      "[[21 24]\n",
      " [47 54]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#元素对应相乘\r\n",
    "E = tf.multiply(A,B)\r\n",
    "print(E)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[ 5 12]\n",
      " [24 36]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 自动求导机制"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "e.g.1 单元函数求导数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "x = tf.Variable(initial_value = 3.)#声明变量与其初始值\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "    y = tf.square(x)\r\n",
    "y_grad = tape.gradient(y, x)#计算y在x处的导数\r\n",
    "print('y: ',y)\r\n",
    "print('y_grad: ',y_grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y:  tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "y_grad:  tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "e.g.2 多元函数求偏导"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "x = tf.constant([[1., 2.],[3., 4.]])\r\n",
    "y = tf.constant([1., 2.])\r\n",
    "w = tf.Variable(initial_value=[[1.],[2.]])\r\n",
    "b = tf.Variable(initial_value=1.)\r\n",
    "\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(x, w) + b - y))\r\n",
    "w_grad,b_grad = tape.gradient(L,[w,b])\r\n",
    "print('L: ',L)\r\n",
    "print('w_grad: ',w_grad)\r\n",
    "print('b_grad: ',b_grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "L:  tf.Tensor(262.0, shape=(), dtype=float32)\n",
      "w_grad:  tf.Tensor(\n",
      "[[144.]\n",
      " [204.]], shape=(2, 1), dtype=float32)\n",
      "b_grad:  tf.Tensor(60.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 线性回归Linear Regression "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 0. 定义数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import numpy as np\r\n",
    "year = np.array([2013,2014,2015,2016,2017],dtype=np.float32)\r\n",
    "price = np.array([12000,14000,15000,16500,17500],dtype=np.float32)\r\n",
    "#归一化\r\n",
    "x = (year-year.min())/(year.max()-year.min())\r\n",
    "y = (price-price.min())/(price.max()-price.min())\r\n",
    "print(x)\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[0.         0.36363637 0.54545456 0.8181818  1.        ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. 初始化\r\n",
    "y=ax+b 目标:找到合适的参数a,b的值，使得模型尽量拟合数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#转换数据类型\r\n",
    "x = tf.constant(x)\r\n",
    "y = tf.constant(y)\r\n",
    "#初始化参数\r\n",
    "a = tf.Variable(initial_value=0.)\r\n",
    "b = tf.Variable(initial_value=0.)\r\n",
    "variables = [a,b]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. 训练\r\n",
    "使用梯度下降法求解"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "epochs = 10000\r\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
    "for i in range(epochs):\r\n",
    "    with tf.GradientTape() as tape:\r\n",
    "        y_pred = a * x + b\r\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\r\n",
    "    grads = tape.gradient(loss,variables)\r\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\r\n",
    "print(a,b)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.9818159> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.05454662>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. 预测"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "x_pred = 2021\r\n",
    "pred = a * x_pred + b\r\n",
    "print(pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(1984.3044, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "11aafd31e056a6f8ee1ae09a20525e039a10b9fd526e7df43c81406d5bd7e71b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}